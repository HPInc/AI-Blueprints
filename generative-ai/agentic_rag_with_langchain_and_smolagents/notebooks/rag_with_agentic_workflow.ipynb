{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-size: 50px;\"> Retrieval-Augmented Generation (RAG) with Agentic Workflow </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements a **Retrieval-Augmented Generation (RAG)** pipeline with an **Agentic Workflow**, using a local **Llama 2** model and **ChromaDB** for intelligent question-answering.  \n",
    "\n",
    "The system dynamically determines whether additional context is needed before generating responses, ensuring higher accuracy and relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Features:\n",
    "- **Llama 2 Model** for high-quality text generation.\n",
    "- **PDF Document Processing** to extract relevant information.\n",
    "- **ChromaDB Vector Store** for efficient semantic search.\n",
    "- **Dynamic Context Retrieval** to improve answer accuracy.\n",
    "- **Two Answering Modes**:\n",
    "  - With RAG (Retrieves relevant document content before responding).\n",
    "  - Without RAG (Directly generates responses)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "- Imports\n",
    "- Configurations\n",
    "- Verify Assets\n",
    "- Model Setup\n",
    "- Loading and Processing the PDF Document\n",
    "- Splitting the Document into Chunks\n",
    "- Initializing the Embedding Model\n",
    "- Computing Embeddings for Document Chunks\n",
    "- Storing Document Embeddings in ChromaDB\n",
    "- Implementing Vector Search Tool\n",
    "- Context Need Assessment\n",
    "- Answer Generation with Agentic RAG\n",
    "- Answer Generation Without RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages if you have not installed them already\n",
    "%pip install -r ../requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries\n",
    "import os\n",
    "import logging                  \n",
    "import warnings   \n",
    "from pathlib import Path\n",
    "\n",
    "# Numerical Computing\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Hugging Face Hub for Model Download\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# LangChain Components\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# Sentence Transformers for Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ChromaDB for Vector Storage\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Transformers \n",
    "import transformers\n",
    "import sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Python warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create logger\n",
    "logger = logging.getLogger(\"notebook_logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", \n",
    "                              datefmt=\"%Y-%m-%d %H:%M:%S\")  \n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_MODEL_PATH = \"/home/jovyan/datafabric/llama2-7b/ggml-model-f16-Q5_K_M.gguf\"\n",
    "PDF_PATH = \"../data/AIStudioDoc.pdf\"\n",
    "# Define text splitting parameters\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "# Define the embedding model name\n",
    "EMBEDDING_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "# Define Chroma database path and collection name\n",
    "CHROMA_DB_PATH = \"./chroma_db\"\n",
    "COLLECTION_NAME = \"document_embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:58:24 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify Assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 15:58:24 - INFO - Local Llama model is properly configured. \n"
     ]
    }
   ],
   "source": [
    "def log_asset_status(asset_path: str, asset_name: str, success_message: str, failure_message: str) -> None:\n",
    "    \"\"\"\n",
    "    Logs the status of a given asset based on its existence.\n",
    "\n",
    "    Parameters:\n",
    "        asset_path (str): File or directory path to check.\n",
    "        asset_name (str): Name of the asset for logging context.\n",
    "        success_message (str): Message to log if asset exists.\n",
    "        failure_message (str): Message to log if asset does not exist.\n",
    "    \"\"\"\n",
    "    if Path(asset_path).exists():\n",
    "        logger.info(f\"{asset_name} is properly configured. {success_message}\")\n",
    "    else:\n",
    "        logger.info(f\"{asset_name} is not properly configured. {failure_message}\")\n",
    "\n",
    "\n",
    "# Check and log status for Local model\n",
    "log_asset_status(\n",
    "    asset_path=LOCAL_MODEL_PATH,\n",
    "    asset_name=\"Local Llama model\",\n",
    "    success_message=\"\",\n",
    "    failure_message=\"Please create and download the required assets in your project on AI Studio if you want to use local model.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîß Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model with the local path and GPU acceleration\n",
    "llm = LlamaCpp(\n",
    "    model_path=LOCAL_MODEL_PATH,\n",
    "    temperature=0.2,\n",
    "    max_tokens=2000,\n",
    "    n_ctx=4096,\n",
    "    top_p=1.0,\n",
    "    verbose=False,\n",
    "    n_gpu_layers=30,  # Utilize some available GPU layers\n",
    "    n_batch=1024,      # Optimize batch size for parallel processing\n",
    "    f16_kv=True,      # Enable half-precision for key/value cache\n",
    "    use_mlock=True,   # Lock memory to prevent swapping\n",
    "    use_mmap=True     # Utilize memory mapping for faster loading\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìÑ Loading and Processing the PDF Document\n",
    "\n",
    "To enable context-aware question-answering, we load a **PDF document**, extract its content, and split it into manageable chunks for efficient retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PDF from: ../data/AIStudioDoc.pdf\n",
      "Successfully loaded 8 document(s) from the PDF.\n"
     ]
    }
   ],
   "source": [
    "# --- Load the PDF Document ---\n",
    "\n",
    "# Define the PDF file path\n",
    "print(f\"Loading PDF from: {PDF_PATH}\")\n",
    "\n",
    "# Load the PDF document\n",
    "pdf_loader = PyPDFLoader(PDF_PATH)\n",
    "documents = pdf_loader.load()\n",
    "\n",
    "print(f\"Successfully loaded {len(documents)} document(s) from the PDF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚úÇÔ∏è Splitting the Document into Chunks\n",
    "\n",
    "Since large documents are difficult to process in full, we split the text into **small overlapping chunks** of approximately **500 characters**. These chunks will later be embedded and stored in ChromaDB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully split PDF into 8 text chunks.\n"
     ]
    }
   ],
   "source": [
    "# --- Split the PDF Content into Manageable Chunks ---\n",
    "\n",
    "# Initialize the text splitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "# Split the PDF content into chunks\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Successfully split PDF into {len(docs)} text chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Initializing the Embedding Model\n",
    "\n",
    "To convert text into numerical representations for efficient similarity search, we use **all-MiniLM-L6-v2** from `sentence-transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded embedding model: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize the Embedding Model ---\n",
    "\n",
    "# Load the embedding model\n",
    "embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)\n",
    "\n",
    "print(f\"Successfully loaded embedding model: {EMBEDDING_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† Computing Embeddings for Document Chunks\n",
    "\n",
    "Each chunk is converted into a **vector representation** using our embedding model. This allows us to perform **semantic similarity searches** later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully computed embeddings for each text chunk.\n",
      "Embeddings Shape: (8, 384)\n"
     ]
    }
   ],
   "source": [
    "# --- Compute Embeddings for Each Text Chunk ---\n",
    "\n",
    "# Extract text content from each chunk\n",
    "doc_texts = [doc.page_content for doc in docs]\n",
    "\n",
    "# Compute embeddings for the extracted text chunks\n",
    "document_embeddings = embedding_model.encode(doc_texts, convert_to_numpy=True)\n",
    "\n",
    "# Display the result\n",
    "print(\"Successfully computed embeddings for each text chunk.\")\n",
    "print(f\"Embeddings Shape: {document_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üóÑÔ∏è Storing Document Embeddings in ChromaDB\n",
    "\n",
    "We initialize **ChromaDB**, a high-performance **vector database**, and store our computed embeddings to enable efficient retrieval of relevant text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Insert of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Insert of existing embedding ID: 3\n",
      "Add of existing embedding ID: 4\n",
      "Insert of existing embedding ID: 4\n",
      "Add of existing embedding ID: 5\n",
      "Insert of existing embedding ID: 5\n",
      "Add of existing embedding ID: 6\n",
      "Insert of existing embedding ID: 6\n",
      "Add of existing embedding ID: 7\n",
      "Insert of existing embedding ID: 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully populated Chroma database with document embeddings.\n"
     ]
    }
   ],
   "source": [
    "# --- Initialize and Populate the Chroma Vector Database ---\n",
    "\n",
    "# Initialize Chroma client\n",
    "chroma_client = chromadb.PersistentClient(path=CHROMA_DB_PATH)\n",
    "collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)\n",
    "\n",
    "# Add document embeddings to the Chroma collection\n",
    "for i, embedding in enumerate(document_embeddings):\n",
    "    collection.add(\n",
    "        ids=[str(i)],  # Chroma requires string IDs\n",
    "        embeddings=[embedding.tolist()],\n",
    "        metadatas=[{\"text\": doc_texts[i]}]\n",
    "    )\n",
    "\n",
    "print(\"Successfully populated Chroma database with document embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîé Implementing Vector Search Tool\n",
    "\n",
    "To retrieve relevant text passages from the database, we define a **vector search function** that finds the most relevant chunks based on a user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the Vector Search Tool ---\n",
    "def vector_search_tool(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Searches the Chroma database for relevant text chunks based on the query.\n",
    "    Computes the query embedding, retrieves the top 5 most relevant text chunks,\n",
    "    and returns them as a formatted string.\n",
    "    \"\"\"\n",
    "    # Compute the query embedding\n",
    "    query_embedding = embedding_model.encode(query, convert_to_numpy=True).tolist()\n",
    "    \n",
    "    # Define the number of nearest neighbors to retrieve\n",
    "    TOP_K = 5\n",
    "    \n",
    "    # Perform the search in the Chroma database\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=TOP_K\n",
    "    )\n",
    "    \n",
    "    # Retrieve and format the corresponding text chunks\n",
    "    retrieved_chunks = [metadata[\"text\"] for metadata in results[\"metadatas\"][0]]\n",
    "    return \"\\n\\n\".join(retrieved_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ü§ñ Context Need Assessment\n",
    "\n",
    "Instead of always retrieving context, we determine if the query **requires external document context** before generating a response. This creates an agentic workflow that makes autonomous decisions to complete the task at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define the Meta-Evaluation Function ---\n",
    "def needs_context(query: str) -> bool:\n",
    "    \"\"\"\n",
    "    Determines if additional context from an external document is required to generate an accurate and detailed answer.\n",
    "    Returns True if context is needed (response contains \"YES\"), False otherwise.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's query to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if external context is required, False otherwise.\n",
    "    \"\"\"\n",
    "    meta_prompt = (\n",
    "        \"Based on the following query, decide if additional context from an external document is needed \"\n",
    "        \"to generate an accurate and detailed answer. Have a tendency to use an external document if the query is not a very familiar topic. If in doubt, assume context is required and answer 'YES'.\\n\"\n",
    "        \"Answer with a single word: YES if additional context from an external document would be helpful to answer the query, \"\n",
    "        \"or NO if not. Do not say anything other than YES or NO.\\n\"\n",
    "        f\"Query: {query}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    meta_response = llm.invoke(meta_prompt)\n",
    "    print(\"Meta Response (is external document retrieval necessary?):\", meta_response)\n",
    "    return \"YES\" in meta_response.upper()\n",
    "\n",
    "\n",
    "# --- Define the Main Answer Generation Function with RAG (Retrieve and Generate) ---\n",
    "def generate_answer_with_agentic_rag(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a detailed and accurate answer to the user's query by using context when needed.\n",
    "    If additional context is required, it is retrieved from the vector store and included in the prompt.\n",
    "    If not, the answer is generated using the query alone.\n",
    "\n",
    "    Args:\n",
    "        query (str): The user's query to answer.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated answer based on the query.\n",
    "    \"\"\"\n",
    "    if needs_context(query):\n",
    "        # Retrieve additional context from the vector store\n",
    "        context = vector_search_tool(query)\n",
    "        \n",
    "        # Construct the enriched prompt with the additional context\n",
    "        enriched_prompt = (\n",
    "            \"Here is additional context from our document:\\n\"\n",
    "            f\"{context}\\n\\n\"\n",
    "            f\"Based on this context and the query: {query}\\n\"\n",
    "            \"Please provide a detailed and accurate answer.\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "        final_response = llm.invoke(enriched_prompt)\n",
    "    else:\n",
    "        # Generate an answer using the original query directly\n",
    "        direct_prompt = (\n",
    "            \"Please provide a detailed and accurate answer to the following query:\\n\"\n",
    "            f\"{query}\\n\"\n",
    "            \"Answer:\"\n",
    "        )\n",
    "        final_response = llm.invoke(direct_prompt)\n",
    "    \n",
    "    return final_response\n",
    "\n",
    "\n",
    "# --- Define the Answer Generation Function without RAG ---\n",
    "def generate_answer_without_rag(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Generates a detailed and accurate answer to the user's query without using any additional context from external documents.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user's query to answer.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated answer based on the query.\n",
    "    \"\"\"\n",
    "    direct_prompt = (\n",
    "        \"Please provide a detailed and accurate answer to the following query:\\n\"\n",
    "        f\"{query}\\n\"\n",
    "        \"Answer:\"\n",
    "    )\n",
    "    final_response = llm.invoke(direct_prompt)\n",
    "    \n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üí° Answer Generation with Agentic RAG\n",
    "\n",
    "If additional context is needed, the model retrieves **relevant document chunks** and incorporates them into the response prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: What are the key features of Z by HP AI Studio?\n",
      "Meta Response (is external document retrieval necessary?):  YES\n",
      "\n",
      "Final Answer:\n",
      " Based on the provided context, Z by HP AI Studio is a standalone application designed for data scientists and engineers that offers several key features to enhance their productivity and collaboration. Here are some of the key features of Z by HP AI Studio:\n",
      "1. Data Connectors: Z by HP AI Studio allows users to connect to multiple data-stores across local and cloud networks, making it easier to access the correct data and packages wherever they are.\n",
      "2. Local Computation: The platform enables users to perform all their computations locally without interruption, providing a more manageable development, data, and model environment.\n",
      "3. Monitoring: AI Studio runs the tools users select natively, allowing them to use all their favorite DS applications directly from the application. Users can view the memory, cores, and GPU required for each workspace to run optimally and determine which workstation best suits their needs.\n",
      "4. Project Management: Z by HP AI Studio provides a centralized platform for managing projects, including creating new services, reusing workspaces, and monitoring projects. Users can view the status of their projects in real-time and take action as needed.\n",
      "5. Collaboration: The platform enables users to invite team members to collaborate on projects, making it easier to work together on complex data science tasks. Users can also share projects with other stakeholders for further collaboration.\n",
      "6. Data Fabric: Z by HP AI Studio offers a built-in data fabric that allows users to mount their data directories in the container, enabling them to use the data to train machine learning models and run other high-velocity tests. Users can create new datasets and specify the download and upload settings for their datasets.\n",
      "7. Notebooks: The platform provides a notebooks page where users can view and manage their running notebooks. They can create new services, name them, and save them for future use.\n",
      "8. Reusing Workspaces: Users can reuse workspaces to quickly access projects they have worked on previously. They can search for specific workspaces using the search bar or browse through previously used workspaces.\n",
      "9. Memory, Cores, and GPU Requirements: Z by HP AI Studio provides users with real-time information about their project's memory, CPU, and GPU consumption. This helps users optimize their projects for better performance and efficiency.\n",
      "10. Expert Support: The platform offers expert support to help users get the most out of their Z by HP AI Studio experience. Users can connect with experts directly from the application or visit the company's website for more information.\n",
      "\n",
      "In summary, Z by HP AI Studio is designed to provide data scientists and engineers with a comprehensive platform that simplifies collaboration, streamlines project management, and enhances productivity. With its key features, users can connect to multiple data sources, perform computations locally, monitor their projects in real-time, collaborate with team members, and leverage the built-in data fabric for efficient data management.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the key features of Z by HP AI Studio?\"\n",
    "print(\"User Query:\", query)\n",
    "final_answer = generate_answer_with_agentic_rag(query)\n",
    "print(\"\\nFinal Answer:\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ‚ö° Answer Generation Without RAG\n",
    "\n",
    "In this case, we generate an answer without using RAG to show the difference between 2 answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Query: What are the key features of Z by HP AI Studio?\n",
      "\n",
      "Final Answer:\n",
      "\n",
      "Z by HP AI Studio is an all-in-one AI development platform designed to help developers build, train, and deploy AI models more efficiently. Here are some of its key features:\n",
      "1. User-friendly Interface: Z by HP AI Studio provides a user-friendly interface that makes it easy for developers to build, train, and deploy AI models without requiring extensive knowledge of AI or machine learning.\n",
      "2. Pre-built Models: The platform comes with pre-built models for various tasks such as image classification, object detection, and natural language processing. Developers can use these models as a starting point and customize them to suit their needs.\n",
      "3. AutoML: Z by HP AI Studio offers automated machine learning (AutoML) capabilities that enable developers to train AI models without writing any code. They can simply select the type of model they want to build, choose the data they want to use, and the platform will automatically generate the code for them.\n",
      "4. Data Science Workbench: The platform provides a data science workbench that allows developers to manage their data assets, perform data preprocessing, and create data pipelines. They can also use the workbench to visualize their data and explore patterns and insights.\n",
      "5. Integrated Development Environment (IDE): Z by HP AI Studio offers an integrated development environment (IDE) that allows developers to write, test, and debug their AI code in a single platform. The IDE includes features such as syntax highlighting, code completion, and debugging tools.\n",
      "6. Collaboration Tools: The platform provides collaboration tools that enable developers to work with their teams more efficiently. They can share models, data, and projects with their colleagues and collaborate on AI development projects.\n",
      "7. Cloud-based Deployment: Z by HP AI Studio allows developers to deploy their AI models directly to the cloud without requiring any additional infrastructure. They can use the platform's built-in deployment tools to deploy their models to popular cloud platforms such as AWS, Google Cloud, and Azure.\n",
      "8. Model Monitoring and Maintenance: The platform provides features for monitoring and maintaining AI models in production environments. Developers can track model performance, detect anomalies, and update their models with new data to ensure they remain accurate and relevant.\n",
      "9. Integration with HP's AI Hardware: Z by HP AI Studio is designed to work seamlessly with HP's AI hardware such as the HP ZBook 15 G6 mobile workstation, which provides powerful processing and graphics capabilities for AI development and deployment.\n",
      "10. Scalability: The platform is designed to scale to meet the needs of large enterprises and organizations. Developers can use Z by HP AI Studio to build and deploy AI models at scale, without requiring any additional infrastructure or expertise.\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the key features of Z by HP AI Studio?\"\n",
    "print(\"User Query:\", query)\n",
    "final_answer = generate_answer_without_rag(query)\n",
    "print(\"\\nFinal Answer:\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-27 16:02:58 - INFO - Notebook execution completed.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Notebook execution completed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Built with ‚ù§Ô∏è using [**Z by HP AI Studio**](https://zdocs.datascience.hp.com/docs/aistudio/overview)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
