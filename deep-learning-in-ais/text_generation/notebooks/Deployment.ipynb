{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ba878a3",
   "metadata": {},
   "source": [
    "# Deployment\n",
    "In this notebook, our objective is to register the model using MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb9c16a",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "- Install requirements and Imports Dependencies\n",
    "- Configurations\n",
    "- Get Text Data\n",
    "- Loading Model\n",
    "- MLFlow - Register Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3f7a87-1faf-4f44-8b89-b7be46ffcc2b",
   "metadata": {},
   "source": [
    "# Imports Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c12dca25-6a47-4674-aa23-8dd9e7afbdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Third-Party Libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# MLflow for Experiment Tracking and Model Management\n",
    "import mlflow\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.models import ModelSignature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc18236",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "798ce8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31c96f9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define global experiment and run names to be used throughout the notebook\n",
    "REGISTER_NAME = \"Shakespeare_Model\"\n",
    "EXPERIMENT_NAME = \"Shakespeare Text Generation\"\n",
    "RUN_NAME = \"Shakespeare_main\"\n",
    "\n",
    "\n",
    "# Set up the paths\n",
    "DATA_PATH = \"../shakespeare.txt\"\n",
    "MODEL_PATH = 'models/dict_torch_rnn_model.pt'\n",
    "MODEL_DECODER_PATH = \"models/decoder.pt\"\n",
    "MODEL_ENCODER_PATH = \"models/encoder.pt\"\n",
    "\n",
    "# Set up the chunk separator for text processing\n",
    "CHUNK_SEPARATOR = \"\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54cccd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Create logger ===\n",
    "logger = logging.getLogger(\"deployment-notebook\")\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(levelname)s - %(message)s\", \n",
    "                             datefmt=\"%Y-%m-%d %H:%M:%S\") \n",
    "\n",
    "stream_handler = logging.StreamHandler()\n",
    "stream_handler.setFormatter(formatter)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.propagate = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fec8b615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 11:39:39 - INFO - Notebook execution started.\n"
     ]
    }
   ],
   "source": [
    "logger.info('Notebook execution started.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89849458-6614-41f7-916c-ca38aec363eb",
   "metadata": {},
   "source": [
    "# Get Text Data\n",
    "This is the text we'll use as a basis for our generations: let's try to generate 'Shakespearean' texts.\n",
    "\n",
    "This text is from Shakespeare's Sonnet 1. It's one of the 154 sonnets written by William Shakespeare that were first published in 1609. This particular sonnet, like many others, discusses themes of beauty, procreation, and the transient nature of life, urging the beautiful to reproduce so their beauty can live on through their offspring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4e89ba8-1e88-450e-bea4-4a0a99793bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA_PATH,'r',encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "all_characters = set(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e8c88-c69f-416a-9ac0-04f049988fc2",
   "metadata": {},
   "source": [
    "# Loading Model\n",
    "The models are available at the [models](models/) folder, where:\n",
    " - [Tensorflow Jupyter Notebook](RNN_for_text_generation_TF.ipynb): `tf_rnn_model.h5`\n",
    " - [PyTorch Jupyter Notebook](RNN_for_text_generation_Torch.): `dict_torch_rnn_model.pt`. Also includes the `decoder.pt` and `encoder.pt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "146ac375-2b3d-431c-a186-d1372e106cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    def __init__(self, decoder, encoder, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5, use_gpu=False):\n",
    "        \"\"\"Initializes CharModel\n",
    "\n",
    "        Args:\n",
    "            decoder: Assigns a unique integer to each character in a dictionary format\n",
    "            encoder : Reverses the decoder dictionary, providing a mapping from characters to their respective assigned integers.\n",
    "            all_chars: Set of unique characters found in the text.\n",
    "            num_hidden: Number of hidden layers. Defaults to 256.\n",
    "            num_layers: Number of layers. Defaults to 4.\n",
    "            drop_prob: Regularization technique to prevent overfitting. Defaults to 0.5.\n",
    "            use_gpu: If the model uses GPU. Defaults to False.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            super().__init__()\n",
    "            self.drop_prob = drop_prob\n",
    "            self.num_layers = num_layers\n",
    "            self.num_hidden = num_hidden\n",
    "            self.use_gpu = use_gpu\n",
    "            \n",
    "            self.all_chars = all_chars\n",
    "            self.decoder = torch.load(decoder)\n",
    "            self.encoder = torch.load(encoder)\n",
    "            \n",
    "            self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "            self.dropout = nn.Dropout(drop_prob)\n",
    "            self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "\n",
    "            logger.info(\"CharModel Initialized successfully\")\n",
    "    \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error Initializing CharModel: {str(e)}\")\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"Implementation of the CharModel logic, in which, the input passes through every step of the arquiteture\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor with shape (batch size and senquency length) containing character indices.\n",
    "            hidden: Tuple containing the inicial hidden states of the CharModel each with shape (batch size and senquency length).\n",
    "\n",
    "        Returns:\n",
    "            final_out: Output tensor representing the predicted logits for each character in the sequence.\n",
    "            hidden: Tuple containing the final hidden states of the CharModel.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            lstm_output, hidden = self.lstm(x, hidden)       \n",
    "            drop_output = self.dropout(lstm_output)\n",
    "            drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "            final_out = self.fc_linear(drop_output)\n",
    "            \n",
    "            return final_out, hidden\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error implementing CharModel logic: {str(e)}\")\n",
    "    \n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        \"\"\"\n",
    "        Initializes and returns the initial hidden state for a recurrent neural network .\n",
    "\n",
    "        This method creates zero-filled tensors for the hidden state (h_0) and cell state (c_0), \n",
    "        supporting GPU execution if `self.use_gpu` is set to True.\n",
    "\n",
    "        Args:\n",
    "            batch_size: The number of sequences in the input batch, used to determine the tensor dimensions.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: A tuple containing the hidden state and cell state tensors \n",
    "            with shape (num_layers, batch_size, num_hidden). Returns None if an exception occurs, and logs the error.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.use_gpu:\n",
    "                hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
    "                        torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
    "            else:\n",
    "                hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                        torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "            \n",
    "            return hidden\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error hidding state: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df33904-7221-4be1-846d-6f7a24c58cb9",
   "metadata": {},
   "source": [
    "# MLFlow - Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c2d2faa-e9d5-45b3-a862-8c6136bbe625",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        \"\"\"\n",
    "        Loads the model and associated artifacts (encoder, decoder) into memory.\n",
    "\n",
    "        Args:\n",
    "            context: MLflow context containing paths to model artifacts.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model = CharModel(\n",
    "                            all_chars=all_characters,\n",
    "                            num_hidden=512,\n",
    "                            num_layers=3,\n",
    "                            drop_prob=0.5,\n",
    "                            use_gpu=False,\n",
    "                            decoder=context.artifacts['decoder'],\n",
    "                            encoder=context.artifacts['encoder']\n",
    "                                            \n",
    "                        )\n",
    "\n",
    "            self.model.load_state_dict(torch.load(context.artifacts['model_state_dict']))\n",
    "            self.model.eval()\n",
    "            logger.info(\"Loading context done successfully\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading context: {str(e)}\")\n",
    "\n",
    "    def one_hot_encoder(self, encoded_text, num_uni_chars):\n",
    "        \"\"\"\n",
    "        Convert categorical data into a fixed-size vector of numerical values.\n",
    "\n",
    "        Args:\n",
    "            encoded_text: Batch of encoded text.\n",
    "            num_uni_chars: Number of unique characters\n",
    "\n",
    "        \"\"\"\n",
    "        try:\n",
    "            one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "            one_hot = one_hot.astype(np.float32)\n",
    "            one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "            one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "            \n",
    "            return one_hot\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error converting categorical data: {str(e)}\")\n",
    "\n",
    "    def predict_next_char(self, char, hidden=None, k=3):\n",
    "        \"\"\"\n",
    "        Predicts the next character given an input character and the current hidden state.\n",
    "\n",
    "        This method encodes the input character, feeds it through the trained character-level \n",
    "        language model (e.g., LSTM), and samples from the top-k most probable characters \n",
    "        to determine the next one. It also returns the updated hidden state for sequential prediction.\n",
    "\n",
    "        Args:\n",
    "            char: The input character to start prediction from.\n",
    "            hidden: Current hidden state of the model. Each tensor has shape (num_layers, batch_size, num_hidden).\n",
    "                If None, a new hidden state should be initialized before calling this method.\n",
    "            k: Number of top predictions to sample from.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the predicted next character and the updated hidden state.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            encoded_text = self.model.encoder[char]\n",
    "            encoded_text = np.array([[encoded_text]])\n",
    "            encoded_text = self.one_hot_encoder(encoded_text, len(self.model.all_chars))\n",
    "            inputs = torch.from_numpy(encoded_text)\n",
    "            inputs = inputs.cpu()\n",
    "                \n",
    "            hidden = tuple([state.data for state in hidden])\n",
    "            lstm_out, hidden = self.model(inputs, hidden)    \n",
    "            probs = F.softmax(lstm_out, dim=1).data\n",
    "            probs = probs.cpu()\n",
    "\n",
    "            \n",
    "            probs, index_positions = probs.topk(k)        \n",
    "            index_positions = index_positions.numpy().squeeze()\n",
    "            probs = probs.numpy().flatten()\n",
    "            probs = probs/probs.sum()\n",
    "            char = np.random.choice(index_positions, p=probs)\n",
    "\n",
    "            return self.model.decoder[char], hidden\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error predicting next char: {str(e)}\")\n",
    "\n",
    "    def generate_text(self, seed, size, k=3):\n",
    "        \"\"\"\n",
    "        Generates a sequence of text using the trained character-level language model.\n",
    "\n",
    "        Starting from a seed string, this method uses the model to predict the next character\n",
    "        one at a time, feeding each predicted character back into the model. It continues\n",
    "        this process until the desired output length is reached.\n",
    "\n",
    "        Args:\n",
    "            seed: The initial sequence of characters used to start the text generation.\n",
    "            size: The number of characters to generate after the seed.\n",
    "            k: Number of top character predictions to consider for sampling at each step.\n",
    "\n",
    "        Returns:\n",
    "            The full generated text including the seed and the newly predicted characters.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.model.cpu()\n",
    "                \n",
    "            self.model.eval()\n",
    "            output_chars = [c for c in seed]\n",
    "            hidden = self.model.hidden_state(1)\n",
    "            \n",
    "            for char in seed:\n",
    "                char, hidden = self.predict_next_char(char, hidden, k=k)\n",
    "        \n",
    "            output_chars.append(char)\n",
    "            for i in range(size):\n",
    "                char, hidden = self.predict_next_char(output_chars[-1], hidden, k=k)\n",
    "                output_chars.append(char)\n",
    "                \n",
    "            return ''.join(output_chars)\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating text: {str(e)}\")\n",
    "        \n",
    "    def predict(self, context, model_input):\n",
    "        \"\"\"\n",
    "        Runs inference using the loaded model and input data.\n",
    "\n",
    "        Args:\n",
    "            context: The MLflow context object.\n",
    "            model_input : A dictionary containing 'seed' and 'size' keys.\n",
    "\n",
    "        Returns:\n",
    "             The output from the model containing the predicted text.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            initial_word = model_input['initial_word'][0]\n",
    "            size = model_input['size'][0]\n",
    "            output = self.generate_text(seed=initial_word, size=size)\n",
    "            \n",
    "            return output\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error predicting text: {str(e)}\")\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, model_state_dict, decoder, encoder, demo_folder=\"../demo\"): \n",
    "        \"\"\"\n",
    "        Logs the model to MLflow, including artifacts, dependencies, and input/output signatures.\n",
    "\n",
    "        Args:\n",
    "            model_state_dict: Path where the model is saved before logging.\n",
    "            decoder: Assigns a unique integer to each character in a dictionary format\n",
    "            encoder : Reverses the decoder dictionary, providing a mapping from characters to their respective assigned integers.\n",
    "            demo_folder: Path to the folder containing the compiled demo UI. Defaults to \"demo\".\".\n",
    "        \"\"\"\n",
    "        try:\n",
    "            input_schema = Schema(\n",
    "                [\n",
    "                    ColSpec(\"string\", \"initial_word\"),\n",
    "                    ColSpec(\"long\", \"size\")\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            output_schema = Schema(\n",
    "                [\n",
    "                    ColSpec(\"string\", \"generated_text\")\n",
    "                ]\n",
    "            )\n",
    "        \n",
    "            signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "                \n",
    "            requirements = [\n",
    "                \"torch\",\n",
    "                \"numpy\"\n",
    "            ]\n",
    "            mlflow.pyfunc.log_model(\n",
    "                model_state_dict,\n",
    "                python_model=cls(),\n",
    "                artifacts={\n",
    "                    \"model_state_dict\": model_state_dict, \n",
    "                    'decoder': decoder, \n",
    "                    'encoder': encoder, \n",
    "                    \"demo\": demo_folder},\n",
    "                signature=signature,\n",
    "                pip_requirements=requirements\n",
    "            )\n",
    "            logger.info(\"Logging model to MLflow done successfully\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error logging model to MLflow: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88808f17-797b-4273-86a8-ec4be7b67bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/phoenix/mlflow/238715752359479816', creation_time=1743590713735, experiment_id='238715752359479816', last_update_time=1743590713735, lifecycle_stage='active', name='Shakespeare Text Generation', tags={}>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(experiment_name= EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0e28b95-5dd5-4f36-adb3-258a85edc5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = MODEL_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbf659ed-12e3-4941-b46f-d8cacfdfa5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_name = REGISTER_NAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "344da6ed-dd4a-4063-86ac-f6f0168ffb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 11:39:39 - INFO - Run's Artifact URI: /phoenix/mlflow/238715752359479816/e3adf895dc1e4e50977177e26a631b20/artifacts\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b30cf28fa144192a9fce619a58fa056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9344805b0717440bb6fe000e303046e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96e087655c4242389ab399c045e5ffed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a77b542a1f43425c86346c455d6a140f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 11:39:40 - INFO - Logging model to MLflow done successfully\n",
      "Registered model 'Shakespeare_Model' already exists. Creating a new version of this model...\n",
      "Created version '12' of model 'Shakespeare_Model'.\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name = RUN_NAME) as run:\n",
    "    logger.info(f\"Run's Artifact URI: {run.info.artifact_uri}\")\n",
    "    RNNModel.log_model(model_state_dict, MODEL_DECODER_PATH, MODEL_ENCODER_PATH)\n",
    "    mlflow.register_model(model_uri = f\"runs:/{run.info.run_id}/{model_state_dict}\", name=register_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bc4e7fa-fda9-422d-bff6-b077b6c8d3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client = mlflow.MlflowClient()\n",
    "model_metadata = client.get_latest_versions(register_name, stages=[\"None\"])\n",
    "latest_model_version = model_metadata[0].version\n",
    "latest_model_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923cf27e-2a84-469c-8fe7-ab2508a4a4d4",
   "metadata": {},
   "source": [
    "## Testing registered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2caeb0bd-e530-461f-a30a-1a918f85a63c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-11 11:39:41 - INFO - CharModel Initialized successfully\n",
      "2025-04-11 11:39:41 - INFO - Loading context done successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Love of the word\n",
      "    Would that the caln of the sare ant he sare,\n",
      "    The cound the will a mester, will to\n"
     ]
    }
   ],
   "source": [
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/{register_name}/{latest_model_version}\")\n",
    "print(model.predict({\"initial_word\": 'Love ', \"size\": 100}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da37ac2",
   "metadata": {},
   "source": [
    "Built with ❤️ using Z by HP AI Studio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
