{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b3f7a87-1faf-4f44-8b89-b7be46ffcc2b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12dca25-6a47-4674-aa23-8dd9e7afbdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow import MlflowClient\n",
    "from mlflow.types.schema import Schema, ColSpec\n",
    "from mlflow.types import ParamSchema, ParamSpec\n",
    "from mlflow.models import ModelSignature\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89849458-6614-41f7-916c-ca38aec363eb",
   "metadata": {},
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e89ba8-1e88-450e-bea4-4a0a99793bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update the path to were shakespeare.txt are\n",
    "with open('shakespeare.txt','r',encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "all_characters = set(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5e8c88-c69f-416a-9ac0-04f049988fc2",
   "metadata": {},
   "source": [
    "### Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146ac375-2b3d-431c-a186-d1372e106cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, decoder, encoder, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5, use_gpu=False):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.num_layers = num_layers\n",
    "        self.num_hidden = num_hidden\n",
    "        self.use_gpu = use_gpu\n",
    "        \n",
    "        self.all_chars = all_chars\n",
    "        self.decoder = torch.load(decoder)\n",
    "        self.encoder = torch.load(encoder)\n",
    "        \n",
    "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        lstm_output, hidden = self.lstm(x, hidden)       \n",
    "        drop_output = self.dropout(lstm_output)\n",
    "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
    "        final_out = self.fc_linear(drop_output)\n",
    "        \n",
    "        return final_out, hidden\n",
    "    \n",
    "    \n",
    "    def hidden_state(self, batch_size):\n",
    "        if self.use_gpu:\n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
    "        else:\n",
    "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
    "                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
    "        \n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df33904-7221-4be1-846d-6f7a24c58cb9",
   "metadata": {},
   "source": [
    "# MLFlow - Register Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2d2faa-e9d5-45b3-a862-8c6136bbe625",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(mlflow.pyfunc.PythonModel):\n",
    "    \n",
    "    def load_context(self, context):\n",
    "        self.model = CharModel(\n",
    "                        all_chars=all_characters,\n",
    "                        num_hidden=512,\n",
    "                        num_layers=3,\n",
    "                        drop_prob=0.5,\n",
    "                        use_gpu=False,\n",
    "                        decoder=context.artifacts['decoder'],\n",
    "                        encoder=context.artifacts['encoder']\n",
    "                                           \n",
    "                    )\n",
    "\n",
    "\n",
    "        self.model.load_state_dict(torch.load(context.artifacts['model_state_dict']))\n",
    "        self.model.eval()\n",
    "\n",
    "    def one_hot_encoder(self, encoded_text, num_uni_chars):\n",
    "        one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
    "        one_hot = one_hot.astype(np.float32)\n",
    "        one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
    "        one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
    "        \n",
    "        return one_hot\n",
    "\n",
    "    def predict_next_char(self, char, hidden=None, k=3):\n",
    "        encoded_text = self.model.encoder[char]\n",
    "        encoded_text = np.array([[encoded_text]])\n",
    "        encoded_text = self.one_hot_encoder(encoded_text, len(self.model.all_chars))\n",
    "        inputs = torch.from_numpy(encoded_text)\n",
    "        inputs = inputs.cpu()\n",
    "            \n",
    "        hidden = tuple([state.data for state in hidden])\n",
    "        lstm_out, hidden = self.model(inputs, hidden)    \n",
    "        probs = F.softmax(lstm_out, dim=1).data\n",
    "        probs = probs.cpu()\n",
    "\n",
    "        \n",
    "        probs, index_positions = probs.topk(k)        \n",
    "        index_positions = index_positions.numpy().squeeze()\n",
    "        probs = probs.numpy().flatten()\n",
    "        probs = probs/probs.sum()\n",
    "        char = np.random.choice(index_positions, p=probs)\n",
    "    \n",
    "        return self.model.decoder[char], hidden\n",
    "\n",
    "    def generate_text(self, seed, size, k=3):\n",
    "\n",
    "        self.model.cpu()\n",
    "            \n",
    "        self.model.eval()\n",
    "        output_chars = [c for c in seed]\n",
    "        hidden = self.model.hidden_state(1)\n",
    "        \n",
    "        for char in seed:\n",
    "            char, hidden = self.predict_next_char(char, hidden, k=k)\n",
    "    \n",
    "        output_chars.append(char)\n",
    "        for i in range(size):\n",
    "            char, hidden = self.predict_next_char(output_chars[-1], hidden, k=k)\n",
    "            output_chars.append(char)\n",
    "            \n",
    "        return ''.join(output_chars)\n",
    "            \n",
    "        \n",
    "    def predict(self, context, model_input):\n",
    "        initial_word = model_input['initial_word'][0]\n",
    "        size = model_input['size'][0]\n",
    "        output = self.generate_text(seed=initial_word, size=size)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    @classmethod\n",
    "    def log_model(cls, model_state_dict, decoder, encoder, demo_folder=\"demo\"): \n",
    "        input_schema = Schema(\n",
    "            [\n",
    "                ColSpec(\"string\", \"initial_word\"),\n",
    "                ColSpec(\"long\", \"size\")\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        output_schema = Schema(\n",
    "            [\n",
    "                ColSpec(\"string\", \"generated_text\")\n",
    "            ]\n",
    "        )\n",
    "      \n",
    "        signature = ModelSignature(inputs=input_schema, outputs=output_schema)\n",
    "             \n",
    "        requirements = [\n",
    "            \"torch\",\n",
    "            \"numpy\"\n",
    "        ]\n",
    "        mlflow.pyfunc.log_model(\n",
    "            model_state_dict,\n",
    "            python_model=cls(),\n",
    "            artifacts={\"model_state_dict\": model_state_dict, 'decoder': decoder, 'encoder': encoder, \"demo\": demo_folder},\n",
    "            signature=signature,\n",
    "            pip_requirements=requirements\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88808f17-797b-4273-86a8-ec4be7b67bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(experiment_name='Shakespeare Text Generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e28b95-5dd5-4f36-adb3-258a85edc5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state_dict = 'models/dict_torch_rnn_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf659ed-12e3-4941-b46f-d8cacfdfa5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_name = 'Shakespeare_Model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344da6ed-dd4a-4063-86ac-f6f0168ffb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='Shakespeare_main') as run:\n",
    "    print(f\"Run's Artifact URI: {run.info.artifact_uri}\")\n",
    "    RNNModel.log_model(model_state_dict, 'models/decoder.pt', 'models/encoder.pt')\n",
    "    mlflow.register_model(model_uri = f\"runs:/{run.info.run_id}/{model_state_dict}\", name=register_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc4e7fa-fda9-422d-bff6-b077b6c8d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = mlflow.MlflowClient()\n",
    "model_metadata = client.get_latest_versions(register_name, stages=[\"None\"])\n",
    "latest_model_version = model_metadata[0].version\n",
    "latest_model_version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923cf27e-2a84-469c-8fe7-ab2508a4a4d4",
   "metadata": {},
   "source": [
    "#### Testing registered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2caeb0bd-e530-461f-a30a-1a918f85a63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mlflow.pyfunc.load_model(model_uri=f\"models:/{register_name}/{latest_model_version}\")\n",
    "print(model.predict({\"initial_word\": 'Love ', \"size\": 100}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
