{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f56c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nemo.collections.nlp.models import BERTLMModel\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d09b9bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the punkt tokenizer data\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b4414a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few entries of the DataFrame:\n",
      "   Unnamed: 0  Topic                                             Pledge\n",
      "0           0      1  Actually we as an association are still pretty...\n",
      "1           1      1  EFFAT welcomes the Commission Proposal for a R...\n",
      "2           2      1  HOTREC calls for a level playing field and fai...\n",
      "3           3      1  Estonia sees the need to synchronize and harmo...\n",
      "4           4      1  Sphere Travel Club contributes to a flourishin...\n"
     ]
    }
   ],
   "source": [
    "# Load Preprocessed Data\n",
    "DirPpath = Path(os.path.abspath(''))\n",
    "data_path = str(DirPpath.absolute()) + r\"/data/CleanedData_Augmented.csv\"\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Debug: Print the first few entries of the DataFrame\n",
    "print(\"First few entries of the DataFrame:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a72e3471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the pledges on words\n",
    "documents = [i for i in df[\"Pledge\"]]\n",
    "tokens = [nltk.word_tokenize(i) for i in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f9e6430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "[NeMo I 2025-03-12 17:21:21 cloud:68] Downloading from: https://api.ngc.nvidia.com/v2/models/nvidia/nemo/bertlargeuncased/versions/1.0.0rc1/files/bertlargeuncased.nemo to /root/.cache/torch/NeMo/NeMo_1.22.0/bertlargeuncased/ca4ebba9f05a8ffb79845249ca046983/bertlargeuncased.nemo\n",
      "[NeMo I 2025-03-12 17:21:46 common:913] Instantiating model from pre-trained checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-03-12 17:21:58 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
      "    Train config : \n",
      "    data_file: /home/yzhang/data/nlp/bert/47316/hdf5/lower_case_1_seq_len_512_max_pred_80_masked_lm_prob_0.15_random_seed_12345_dupe_factor_5_shard_1472_test_split_10/books_wiki_en_corpus/training/\n",
      "    max_predictions_per_seq: 80\n",
      "    batch_size: 16\n",
      "    shuffle: true\n",
      "    num_samples: -1\n",
      "    num_workers: 2\n",
      "    drop_last: false\n",
      "    pin_memory: false\n",
      "    \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20aaa32bab6a46ed917035300882552c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.34G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-03-12 17:22:27 modelPT:617] Trainer wasn't specified in model constructor. Make sure that you really wanted it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-03-12 17:22:27 modelPT:728] Optimizer config = AdamW (\n",
      "    Parameter Group 0\n",
      "        amsgrad: False\n",
      "        betas: (0.9, 0.999)\n",
      "        capturable: False\n",
      "        differentiable: False\n",
      "        eps: 1e-08\n",
      "        foreach: None\n",
      "        fused: None\n",
      "        lr: 4.375e-05\n",
      "        maximize: False\n",
      "        weight_decay: 0.01\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2025-03-12 17:22:27 lr_scheduler:890] Neither `max_steps` nor `iters_per_batch` were provided to `optim.sched`, cannot compute effective `max_steps` !\n",
      "    Scheduler will not be instantiated !\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2025-03-12 17:22:29 save_restore_connector:249] Model BERTLMModel was successfully restored from /root/.cache/torch/NeMo/NeMo_1.22.0/bertlargeuncased/ca4ebba9f05a8ffb79845249ca046983/bertlargeuncased.nemo.\n",
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-large-uncased\")\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "print(\"Loading model...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#bert_model = BERTLMModel.restore_from(\"/home/jovyan/datafabric/Bertlargeuncased/bertlargeuncased.nemo\", strict=False)\n",
    "bert_model = BERTLMModel.from_pretrained(model_name=\"bertlargeuncased\", strict=False).to(device)\n",
    "print(\"Model loaded successfully.\")                                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6010c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Generate Embeddings in Batches using NeMo BERT model\n",
    "def generate_embeddings_in_batches(texts, tokenizer, model, batch_size=32):\n",
    "    \"\"\"\n",
    "    Converts a list of texts into embeddings using NeMo BERT model in batches.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "        encoded_input = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "        encoded_input = {key: val.to(device) for key, val in encoded_input.items()}\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation for inference\n",
    "            output = model.bert_model(**encoded_input)\n",
    "\n",
    "        # Extract the CLS token representation for embeddings\n",
    "        embeddings = output[:, 0, :].cpu().numpy()  # CLS token representation\n",
    "        all_embeddings.append(embeddings)\n",
    "\n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a03e94c9-6b53-4812-9226-a066896b95cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embedding completed and saved to: ./data/embedded_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Generate & Save Embeddings\n",
    "vectorsLLM = generate_embeddings_in_batches(documents, tokenizer, bert_model, batch_size=16)\n",
    "\n",
    "df_embeddings = pd.DataFrame(vectorsLLM)\n",
    "output_path = \"./data/embedded_data.csv\"\n",
    "df_embeddings.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"✅ Embedding completed and saved to:\", output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3f567d-9b56-4871-a129-47b89ba4c7f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aistudio",
   "language": "python",
   "name": "aistudio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
